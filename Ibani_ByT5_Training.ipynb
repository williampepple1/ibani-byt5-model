{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üåç Ibani-English Translation with ByT5\n",
        "\n",
        "This notebook trains a ByT5 model for English ‚Üî Ibani translation.\n",
        "\n",
        "## üìã Before Running:\n",
        "1. Upload `ibani_eng_training_data.json` to Google Drive\n",
        "2. Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
        "3. Run all cells\n",
        "\n",
        "## ‚è±Ô∏è Expected Time:\n",
        "- 30-60 minutes on GPU\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers datasets accelerate evaluate sacrebleu tensorboard sentencepiece"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import json\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "print(\"‚úÖ All libraries imported!\")\n",
        "print(f\"üêç Python version: {torch.__version__}\")\n",
        "print(f\"üíª GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚öôÔ∏è CONFIGURATION - Modify these if needed\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    # Model (change to byt5-base or byt5-large for better quality)\n",
        "    model_name: str = \"google/byt5-small\"\n",
        "    \n",
        "    # Data path - UPDATE THIS to your Google Drive path\n",
        "    data_path: str = \"/content/drive/MyDrive/ibani_eng_training_data.json\"\n",
        "    \n",
        "    # Output\n",
        "    output_dir: str = \"/content/ibani-byt5-finetuned\"\n",
        "    \n",
        "    # Training parameters\n",
        "    num_train_epochs: int = 10\n",
        "    per_device_train_batch_size: int = 8\n",
        "    per_device_eval_batch_size: int = 8\n",
        "    learning_rate: float = 5e-5\n",
        "    weight_decay: float = 0.01\n",
        "    warmup_steps: int = 500\n",
        "    \n",
        "    # Generation\n",
        "    max_source_length: int = 256\n",
        "    max_target_length: int = 256\n",
        "    \n",
        "    # Evaluation\n",
        "    eval_steps: int = 500\n",
        "    save_steps: int = 500\n",
        "    logging_steps: int = 100\n",
        "    eval_split: float = 0.1\n",
        "    \n",
        "    # Other\n",
        "    seed: int = 42\n",
        "    fp16: bool = torch.cuda.is_available()\n",
        "\n",
        "config = TrainingConfig()\n",
        "print(\"‚úÖ Configuration loaded\")\n",
        "print(f\"üìä Model: {config.model_name}\")\n",
        "print(f\"üìÅ Data: {config.data_path}\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare data\n",
        "print(f\"üìä Loading data from {config.data_path}...\")\n",
        "\n",
        "with open(config.data_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "english_texts = []\n",
        "ibani_texts = []\n",
        "\n",
        "for item in data:\n",
        "    translation = item.get('translation', {})\n",
        "    en_text = translation.get('en', '').strip()\n",
        "    ibani_text = translation.get('ibani', '').strip()\n",
        "    \n",
        "    if en_text and ibani_text:\n",
        "        english_texts.append(en_text)\n",
        "        ibani_texts.append(ibani_text)\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(english_texts):,} translation pairs\")\n",
        "\n",
        "# Create datasets\n",
        "dataset_dict = {'english': english_texts, 'ibani': ibani_texts}\n",
        "dataset = Dataset.from_dict(dataset_dict)\n",
        "split_dataset = dataset.train_test_split(test_size=config.eval_split, seed=42)\n",
        "\n",
        "train_dataset = split_dataset['train']\n",
        "eval_dataset = split_dataset['test']\n",
        "\n",
        "print(f\"üìö Train: {len(train_dataset):,} | Validation: {len(eval_dataset):,}\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "print(f\"ü§ñ Loading {config.model_name}...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.model_name)\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {model.num_parameters():,} parameters\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"translate English to Ibani: {text}\" for text in examples['english']]\n",
        "    targets = examples['ibani']\n",
        "    \n",
        "    model_inputs = tokenizer(inputs, max_length=config.max_source_length, truncation=True, padding=False)\n",
        "    labels = tokenizer(targets, max_length=config.max_target_length, truncation=True, padding=False)\n",
        "    \n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "print(\"üîÑ Preprocessing datasets...\")\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "eval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=eval_dataset.column_names)\n",
        "print(\"‚úÖ Preprocessing complete\")"
      ],
      "metadata": {
        "id": "preprocess"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup training\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
        "bleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    \n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "    \n",
        "    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return {\"bleu\": result[\"score\"]}\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=config.output_dir,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=config.eval_steps,\n",
        "    save_steps=config.save_steps,\n",
        "    logging_steps=config.logging_steps,\n",
        "    learning_rate=config.learning_rate,\n",
        "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
        "    weight_decay=config.weight_decay,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=config.num_train_epochs,\n",
        "    predict_with_generate=True,\n",
        "    fp16=config.fp16,\n",
        "    warmup_steps=config.warmup_steps,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"bleu\",\n",
        "    greater_is_better=True,\n",
        "    push_to_hub=False,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    seed=config.seed,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer initialized\")"
      ],
      "metadata": {
        "id": "setup_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ TRAIN THE MODEL\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(f\"‚è±Ô∏è  {config.num_train_epochs} epochs\")\n",
        "print(f\"üìä {len(train_dataset) // config.per_device_train_batch_size * config.num_train_epochs} total steps\")\n",
        "print(\"\\nThis will take 30-60 minutes on GPU...\\n\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nüéâ Training complete!\")"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "print(f\"üíæ Saving model to {config.output_dir}\")\n",
        "trainer.save_model(config.output_dir)\n",
        "tokenizer.save_pretrained(config.output_dir)\n",
        "\n",
        "# Final evaluation\n",
        "metrics = trainer.evaluate()\n",
        "print(f\"\\nüìä Final BLEU score: {metrics['eval_bleu']:.2f}\")\n",
        "\n",
        "with open(os.path.join(config.output_dir, \"final_metrics.json\"), 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Model saved!\")"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "def translate(text, source_lang=\"en\", target_lang=\"ibani\"):\n",
        "    if source_lang == \"en\":\n",
        "        prompt = f\"translate English to Ibani: {text}\"\n",
        "    else:\n",
        "        prompt = f\"translate Ibani to English: {text}\"\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=256, truncation=True).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_length=256, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "print(\"üß™ Testing translations:\\n\")\n",
        "test_sentences = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"Good morning\",\n",
        "    \"Thank you very much\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    translation = translate(sentence)\n",
        "    print(f\"EN: {sentence}\")\n",
        "    print(f\"IB: {translation}\\n\")"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to Google Drive\n",
        "print(\"üì• Saving model to Google Drive...\")\n",
        "!cp -r /content/ibani-byt5-finetuned /content/drive/MyDrive/\n",
        "print(\"‚úÖ Model saved to Google Drive: /MyDrive/ibani-byt5-finetuned/\")\n",
        "print(\"\\nüéâ All done! Download the folder from Google Drive to use locally.\")"
      ],
      "metadata": {
        "id": "save_to_drive"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
