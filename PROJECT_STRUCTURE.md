# Project Structure

```
ibani-byt5-model/
â”‚
â”œâ”€â”€ ğŸ“Š Data
â”‚   â””â”€â”€ ibani_eng_training_data.json    # Your training dataset (47,804 pairs)
â”‚
â”œâ”€â”€ ğŸ§  Training
â”‚   â””â”€â”€ train.py                        # ByT5 model training script
â”‚
â”œâ”€â”€ ğŸŒ API Server
â”‚   â”œâ”€â”€ app.py                          # FastAPI application
â”‚   â””â”€â”€ test_api.py                     # API testing script
â”‚
â”œâ”€â”€ ğŸ’» CLI Tools
â”‚   â””â”€â”€ translate.py                    # Command-line translation tool
â”‚
â”œâ”€â”€ ğŸ³ Docker
â”‚   â”œâ”€â”€ Dockerfile                      # Container configuration
â”‚   â”œâ”€â”€ docker-compose.yml              # Orchestration config
â”‚   â””â”€â”€ .dockerignore                   # Docker ignore rules
â”‚
â”œâ”€â”€ âš™ï¸ Configuration
â”‚   â”œâ”€â”€ requirements.txt                # Python dependencies
â”‚   â”œâ”€â”€ .env.example                    # Environment variables template
â”‚   â””â”€â”€ .gitignore                      # Git ignore rules
â”‚
â”œâ”€â”€ ğŸš€ Setup Scripts
â”‚   â”œâ”€â”€ setup.bat                       # Windows setup script
â”‚   â””â”€â”€ setup.sh                        # Linux/Mac setup script
â”‚
â”œâ”€â”€ ğŸ“– Documentation
â”‚   â”œâ”€â”€ README.md                       # Main documentation
â”‚   â”œâ”€â”€ USAGE.md                        # Detailed usage guide
â”‚   â”œâ”€â”€ QUICKSTART.md                   # Quick reference
â”‚   â””â”€â”€ PROJECT_STRUCTURE.md            # This file
â”‚
â””â”€â”€ ğŸ“ Generated (after training)
    â”œâ”€â”€ models/
    â”‚   â””â”€â”€ ibani-byt5-finetuned/       # Trained model files
    â”‚       â”œâ”€â”€ config.json
    â”‚       â”œâ”€â”€ pytorch_model.bin
    â”‚       â”œâ”€â”€ tokenizer_config.json
    â”‚       â””â”€â”€ final_metrics.json
    â”‚
    â”œâ”€â”€ logs/                            # Training logs
    â”‚   â””â”€â”€ tensorboard/
    â”‚
    â””â”€â”€ venv/                            # Python virtual environment
```

## Component Overview

### 1. Training Pipeline
```
ibani_eng_training_data.json
         â†“
    train.py (ByT5 fine-tuning)
         â†“
models/ibani-byt5-finetuned/
```

### 2. Inference Options

#### Option A: CLI
```
User Input â†’ translate.py â†’ ByT5 Model â†’ Translation Output
```

#### Option B: API
```
HTTP Request â†’ FastAPI (app.py) â†’ ByT5 Model â†’ JSON Response
```

#### Option C: Docker
```
HTTP Request â†’ Docker Container â†’ FastAPI â†’ ByT5 Model â†’ JSON Response
```

## Data Flow

### Training
1. Load `ibani_eng_training_data.json`
2. Split into train/validation (90/10)
3. Preprocess with ByT5 tokenizer
4. Fine-tune model
5. Evaluate with BLEU score
6. Save to `models/ibani-byt5-finetuned/`

### Inference (API)
1. Client sends POST request to `/translate`
2. FastAPI validates request
3. Model generates translation
4. Return JSON response with translation

### Inference (CLI)
1. User enters text
2. Script loads model
3. Generate translation
4. Display result

## Key Files Explained

### `train.py`
- Loads training data
- Configures ByT5 model
- Handles preprocessing
- Runs training loop
- Saves trained model

**Key Classes:**
- `TrainingConfig`: Training hyperparameters
- `train_model()`: Main training function
- `preprocess_function()`: Data preprocessing
- `compute_metrics()`: BLEU score calculation

### `app.py`
- FastAPI application
- Model loading and caching
- Translation endpoints
- Request/response validation

**Key Endpoints:**
- `GET /health`: Health check
- `POST /translate`: Single translation
- `POST /batch-translate`: Batch translation

### `translate.py`
- Standalone inference script
- Interactive mode
- Batch testing
- Command-line interface

**Key Functions:**
- `load_model()`: Load trained model
- `translate()`: Generate translation
- `interactive_mode()`: Interactive CLI

## Technology Stack

### Core ML
- **ByT5**: Byte-level T5 model
- **PyTorch**: Deep learning framework
- **Transformers**: Hugging Face library

### API & Server
- **FastAPI**: Modern Python web framework
- **Uvicorn**: ASGI server
- **Pydantic**: Data validation

### Deployment
- **Docker**: Containerization
- **Docker Compose**: Multi-container orchestration

### Evaluation
- **SacreBLEU**: Translation quality metric
- **TensorBoard**: Training visualization

## Model Architecture

```
Input Text (English or Ibani)
         â†“
    ByT5 Tokenizer (Byte-level)
         â†“
    ByT5 Encoder (Transformer)
         â†“
    ByT5 Decoder (Transformer)
         â†“
    Output Text (Ibani or English)
```

## Why ByT5?

1. **Byte-level tokenization**: No vocabulary limitations
2. **Preserves special characters**: Handles Ã¡, á¸…, etc. perfectly
3. **Language-agnostic**: No language-specific preprocessing
4. **Low-resource friendly**: Works well with limited data
5. **No OOV tokens**: Every byte sequence is valid

## Deployment Options

### Development
```bash
python app.py
# Local server at http://localhost:8000
```

### Production (Docker)
```bash
docker-compose up -d
# Containerized server with auto-restart
```

### Cloud Deployment
- AWS ECS/Fargate
- Google Cloud Run
- Azure Container Instances
- Kubernetes

## Performance Characteristics

### Training
- **Dataset**: 47,804 translation pairs
- **Model**: ByT5-small (300M parameters)
- **Time**: 30-60 min (GPU) / 4-8 hours (CPU)
- **Memory**: ~4GB GPU / 8GB RAM

### Inference
- **Latency**: 100-500ms per translation (GPU)
- **Throughput**: 10-50 translations/second (GPU)
- **Memory**: ~2GB GPU / 4GB RAM

## Next Steps

1. **Train**: `python train.py`
2. **Test**: `python translate.py --test`
3. **Deploy**: `docker-compose up -d`
4. **Monitor**: Check logs and metrics
5. **Iterate**: Improve based on results

---

For more details, see:
- [README.md](README.md) - Overview and features
- [USAGE.md](USAGE.md) - Detailed usage instructions
- [QUICKSTART.md](QUICKSTART.md) - Quick reference
